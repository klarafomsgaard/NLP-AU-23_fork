{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classroom 5 - Training a Named Entity Recognition Model with a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classroom today focuses on using LSTMs to train a named entity recognition model, which is an example of a \"many-to-many\" recurrent model.\n",
    "\n",
    "**Note**: there are a few exercises to solve in this notebook. As it is the first exercise where I am asking you to implement several code chunks by yourself (though with plenty of guidance), this week I am uploading the solutions in a separate notebook. \n",
    "If you get stuck on something for a long time and your want some help to get unstuck, you can take a look at the other notebook -- but try not do do so unless it is really the last resource (i.e., you have tried hard, asked around, googled stuff, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A very short intro to NER\n",
    "Named entity recognition (NER) also known as named entity extraction, and entity identification is the task of tagging an entity is the task of extracting which seeks to extract named entities from unstructured text into predefined categories such as names, medical codes, quantities or similar.\n",
    "\n",
    "The most common variant is the [CoNLL-20003](https://www.clips.uantwerpen.be/conll2003/ner/) format which uses the categories, person (PER), organization (ORG) location (LOC) and miscellaneous (MISC), which for example denote cases such nationalies. For example:\n",
    "\n",
    "*Hello my name is $Roberta_{PER}$ I live in $Aarhus_{LOC}$ and work at $AU_{ORG}$.*\n",
    "\n",
    "For example, let's see how this works with ```spaCy```. NB: you might need to remember to install a ```spaCy``` model:\n",
    "\n",
    "```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello my name is Roberta. I live in Denmark and work at Aarhus University, I am Italian and today is Wednesday 25th.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello my name is \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Roberta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". I live in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Denmark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and work at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aarhus University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", I am \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Italian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    today\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wednesday 25th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging standards\n",
    "There exist different tag standards for NER. The most used one is the BIO-format which frames the task as token classification denoting inside, outside and beginning of a token. \n",
    "\n",
    "Words marked with *O* are not a named entity. Words with NER tags which start with *B-\\** indicate the start of a multiword entity (i.e. *B-ORG* for the *Aarhus* in *Aarhus University*), while *I-\\** indicate the continuation of a token (e.g. University).\n",
    "\n",
    "    B = Beginning\n",
    "    I = Inside\n",
    "    O = Outside\n",
    "\n",
    "<details>\n",
    "<summary>Q: What other formats and standards are available? What kinds of entities do they make it possible to tag?</summary>\n",
    "<br>\n",
    "You can see more examples on the spaCy documentation for their [different models(https://spacy.io/models/en)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello O\n",
      "my O\n",
      "name O\n",
      "is O\n",
      "Roberta B-GPE\n",
      ". O\n",
      "I O\n",
      "live O\n",
      "in O\n",
      "Denmark B-GPE\n",
      "and O\n",
      "work O\n",
      "at O\n",
      "Aarhus B-ORG\n",
      "University I-ORG\n",
      ", O\n",
      "I O\n",
      "am O\n",
      "Italian B-NORP\n",
      "and O\n",
      "today B-DATE\n",
      "is O\n",
      "Wednesday B-DATE\n",
      "25th I-DATE\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    if t.ent_type:\n",
    "        print(t, f\"{t.ent_iob_}-{t.ent_type_}\")\n",
    "    else:\n",
    "        print(t, t.ent_iob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some challenges with NER\n",
    "While NER is currently framed as above this formulating does contain some limitations. \n",
    "\n",
    "For instance the entity Aarhus University really refers to both the location Aarhus, the University within Aarhus, thus nested NER (N-NER) argues that it would be more correct to tag it in a nested fashion as \\[\\[$Aarhus_{LOC}$\\] $University$\\]$_{ORG}$ (Plank, 2020). \n",
    "\n",
    "Other task also include named entity linking. Which is the task of linking an entity to e.g. a wikipedia entry, thus you have to both know that it is indeed an entity and which entity it is (if it is indeed a defined entity).\n",
    "\n",
    "We will be using Bi-LSTMs to train an NER model on a predifined data set which uses IOB tags of the kind we outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training in batches\n",
    "\n",
    "In previous classes, we discussed stochastic gradient descent on mini-batches as a way to achieve an optimal tradeoff between performance and stability.\n",
    "Let's implement batching!\n",
    "\n",
    "<details>\n",
    "<summary>Reminder: Why might it be a good idea to train on batches, rather than the whole dataset?</summary>\n",
    "<br>\n",
    "These batches are usually small (something like 32 instances at a time) but they have couple of important effects on training:\n",
    "\n",
    "- Batches can be processed in parallel, rather the sequentially. This can result in substantial speed up from computational perspective\n",
    "- Similarly, smaller batch sizes make it easier to fit training data into memory\n",
    "- Lastly,  smaller batch sizes are noisy, meaning that they have a regularizing effect and thus lead to less overfitting.\n",
    "\n",
    "In this notebook, we're going to be using batches of data to train our NER model. To do that, we first have to prepare our batches for training. You can read more about batching in [this blog post](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/).\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this allows us to look one step up in the directory\n",
    "# for importing custom modules from src\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.util import batch\n",
    "from src.LSTM import LSTMModel\n",
    "from src.embedding import gensim_to_torch_embedding\n",
    "\n",
    "# numpy and pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# loading data and embeddings\n",
    "from datasets import load_dataset\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the dataset using the ```load_dataset()``` function we've already seen. Here we take only the training data.\n",
    "\n",
    "When you've downloaded the dataset, you're welcome to save a local copy so that we don't need to constantly download it again everytime the code runs.\n",
    "\n",
    "Q: What do the ```train.features``` values refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conllpp (/home/ucloud/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403d0b9002de4fe5942789c816b0c35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATASET\n",
    "dataset = load_dataset(\"conllpp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]\n",
    "\n",
    "# inspect the dataset\n",
    "train[\"tokens\"][:1]\n",
    "train[\"ner_tags\"][:1]\n",
    "\n",
    "# get number of classes\n",
    "num_classes = train.features[\"ner_tags\"].feature.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use ```gensim``` to get some pretrained word embeddings for the input layer to the model. \n",
    "\n",
    "In this example, we're going to use a GloVe model pretrained on Wikipedia, with 50 dimensions.\n",
    "\n",
    "I've provided a helper function to take the ```gensim``` embeddings and prepare them for ```pytorch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING EMBEDDINGS\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# you can also try: model = gensim.models.KeyedVectors.load_word2vec_format(\"../../../819739/models/english/model.bin\", binary=True) -- note that dimensionality is different\n",
    "embedding_layer, vocab = gensim_to_torch_embedding(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a batch\n",
    "\n",
    "The first thing we want to do is to shuffle our dataset before training. \n",
    "\n",
    "Why might it be a good idea to shuffle the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "shuffled_train = dataset[\"train\"].shuffle(seed=1)\n",
    "validation = dataset[\"validation\"]\n",
    "test = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['13112'], 'tokens': [['\"', 'I', 'would', 'love', 'to', 'speak', 'about', 'everything', ',', '\"', 'said', 'Simpson', ',', 'who', 'vowed', 'after', 'his', 'acquittal', 'to', 'find', 'the', 'killers', 'and', 'offered', 'a', 'substantial', 'reward', '.']], 'pos_tags': [[0, 28, 20, 37, 35, 37, 15, 21, 6, 0, 38, 22, 6, 44, 38, 15, 29, 21, 35, 37, 12, 24, 10, 38, 12, 16, 21, 7]], 'chunk_tags': [[0, 11, 21, 22, 22, 22, 13, 11, 0, 0, 21, 11, 0, 11, 21, 13, 11, 12, 21, 22, 11, 12, 0, 21, 11, 12, 12, 0]], 'ner_tags': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_train[9:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to bundle the shuffled training data into smaller batches of predefined size. I've written a small utility function here to help. \n",
    "\n",
    "<details>\n",
    "<summary>Q: Can you explain how the batch() function works?</summary>\n",
    "<br>\n",
    " Hint: Check out [this link](https://realpython.com/introduction-to-python-generators/).\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Batch function takes the shuffled dataset and splits it into subsets containing the amount of datapoints specified by the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batches_tokens = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "batches_tags = batch(shuffled_train[\"ner_tags\"], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use the ```tokens_to_idx()``` function below on our batches.\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is this function doing? Why is it doing it?</summary>\n",
    "<br>\n",
    "We're making everything lowercase and adding a new, arbitrary token called <UNK> to the vocabulary. This <UNK> means \"unknown\" and is used to replace out-of-vocabulary tokens in the data - i.e. tokens that don't appear in the vocabulary of the pretrained word embeddings.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idx(tokens, vocab=model.key_to_index):\n",
    "    \"\"\"\n",
    "    The function takes the tokens in our dataset and replaces it with the corresponding index from the GloVe vocabulary (vocab).\n",
    "    The function converts the tokens to lowercase to match the form of the vocabulary\n",
    "    If the word/token is not in the vocabulary, it yield the index matching \"UNK\" (unknown) which in this case is 40000\n",
    "    - What does the .get method do?\n",
    "    The get method tries to retrive the index for the token in lowercase\n",
    "    - Why lowercase?\n",
    "    We want the index to be the same for the same word regardless of whether it is in upper or lowercase\n",
    "    e.g. because it is in the beginning of a sentence\n",
    "    \"\"\"\n",
    "    return [vocab.get(t.lower(), vocab[\"UNK\"]) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check below that everything is working as expected by testing it on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using only the first batch\n",
    "batch_tokens = next(batches_tokens)\n",
    "batch_tags = next(batches_tags)\n",
    "batch_tok_idx = [tokens_to_idx(sent) for sent in batch_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[544, 3535], [19605, 947, 790, 176, 421, 46532, 57483, 904], [5330, 7764, 1161, 1644, 400000, 21157, 31, 1101, 7, 477, 3, 174, 93, 107797, 580, 9731, 1, 13, 4970, 5985, 1, 410, 865, 5330, 7764, 1161, 1644, 16, 2], [4825, 3195, 4760, 347581, 314, 2639], [4179, 400000], [65, 93147, 35913, 4762, 926, 8749, 156, 3, 4905, 49, 76, 364, 9, 508, 2], [8455, 4628, 23, 19658, 24], [2859, 16, 12, 0, 2072, 3, 96951, 78, 1307, 1358, 445, 4, 211, 23837, 2], [4020, 22, 614, 1518], [8, 41, 54, 835, 4, 2199, 59, 1174, 1, 8, 16, 3701, 1, 38, 3416, 49, 26, 17544, 4, 596, 0, 8868, 5, 1112, 7, 3667, 6595, 2], [0, 30773, 1890, 10811, 284343, 25, 1905, 4, 126, 6, 0, 1250, 85, 366, 6499, 17, 10974, 226, 34, 73996, 1827, 4, 802, 7, 7657, 410, 17, 17083, 226, 49, 502, 3, 0, 418, 3460, 2], [1193, 43, 30, 7145, 7, 54257, 2523, 5, 7, 668, 4, 1067, 74, 7, 5128, 354, 3, 1817, 1, 11030, 3, 7, 1800, 874, 5, 7, 400000, 2073, 2], [13697], [877, 2457, 1, 290, 561, 58, 241, 24], [37, 1127, 17, 20314, 1411, 1, 106, 0, 54673, 3, 560, 571, 1710, 1, 3, 70219, 10, 441, 1, 84235, 10, 470, 1, 100416, 10, 304, 5, 25907, 10, 344, 1, 18, 16, 2], [0, 1258, 116, 2978, 4, 12288, 3, 29576, 11167, 6, 510, 1010, 5, 396, 918, 1, 42, 1105, 0, 973, 3, 1094, 29576, 11167, 2], [0, 2536, 15, 1450, 13, 177, 6, 347, 202, 6, 2952, 5, 14, 170, 830, 21509, 74, 102, 11402, 7, 8, 1918, 8, 6, 0, 951, 464, 2], [5160, 23891, 1, 0, 276, 876, 6, 7, 4072, 964, 1475, 5, 1739, 2442, 2674, 1, 14, 1602, 3, 12094, 7, 461, 10545, 787, 1, 0, 10545, 283, 3, 15017, 16, 13, 177, 2], [62, 4, 344, 407, 2], [400000, 785, 24025, 23, 2038, 24, 12508, 4045], [2571, 1, 402, 1, 1204, 10, 1, 98, 1, 226, 24, 45], [11933, 45, 19339, 97315, 23, 2693, 5, 12591, 24, 1, 400000, 400000, 23, 3039, 24, 1], [104894, 1, 2238, 400000], [0, 635, 845, 163, 6017, 4461, 1, 38, 40, 26, 126, 1278, 3, 0, 186, 1, 1227, 103, 3139, 37, 195, 1, 3446, 19, 7, 9064, 2], [807, 3, 4989, 45, 348076, 367680, 400000, 400000, 400000, 2], [8, 150, 252, 57, 1731, 35, 1347, 744, 2], [8, 61, 0, 23, 1269, 24, 1215, 43, 9781, 137, 60, 378, 14, 120, 7, 3137, 3, 818, 5798, 120, 7, 9330, 1, 8, 18, 295, 2], [1147, 400000], [400000, 6169, 400000, 23, 2238, 24, 47381, 72615], [5680, 2], [2162, 11, 474, 8602, 52386, 10, 692, 249, 2], [297523, 188812, 10750, 23, 695, 24, 142151]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect token index\n",
    "print(batch_tok_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with document classification, our model needs to take input sequences of a fixed length. To get around this we do a couple of different steps.\n",
    "\n",
    "- Find the length of the longest sequence in the batch\n",
    "- Pad shorter sequences to the max length using an arbitrary token like <PAD>\n",
    "- Give the <PAD> token a new label ```9``` to differentiate it from the other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute length of longest sequence/sentence in batch\n",
    "batch_max_len = max([len(s) for s in batch_tok_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Can you figure out the logic of what is happening in the next two cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of ones for all sequences in the array, matching the longest sequence in the dataset\n",
    "batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len))\n",
    "\n",
    "# Create corresponding labels for each token index to be able to discriminate later\n",
    "batch_labels = 9 * np.ones((batch_size, batch_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9., 9., 9., ..., 9., 9., 9.],\n",
       "       [9., 9., 9., ..., 9., 9., 9.],\n",
       "       [9., 9., 9., ..., 9., 9., 9.],\n",
       "       ...,\n",
       "       [9., 9., 9., ..., 9., 9., 9.],\n",
       "       [9., 9., 9., ..., 9., 9., 9.],\n",
       "       [9., 9., 9., ..., 9., 9., 9.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data to the numpy array\n",
    "for i in range(batch_size):\n",
    "    tok_idx = batch_tok_idx[i] # Get the array i in the array of token indices\n",
    "    tags = batch_tags[i] # Get tags for array i\n",
    "    size = len(tok_idx) # Get the length of the array \n",
    "\n",
    "    batch_input[i][:size] = tok_idx #replace part of the arrays of zeros with the right token indices -> This way, all sequences have the same size, with the surplus elements being zeros\n",
    "    batch_labels[i][:size] = tags # Raplce the 9s with the right tagt to their corresponding indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to convert the arrays into ```pytorch``` tensors, ready for the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all data are indices, we convert them to torch LongTensors (integers)\n",
    "batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(\n",
    "    batch_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now batched and processed, we want to run it through our RNN the same way as when we trained a classifier.\n",
    "\n",
    "Q: Why is ```output_dim = num_classes + 1```? \n",
    "A: Because it is zero-index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = LSTMModel(\n",
    "    embedding_layer=embedding_layer, output_dim=num_classes + 1, hidden_dim_size=256\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "X = batch_input\n",
    "y = model(X)\n",
    "\n",
    "loss = model.loss_fn(outputs=y, labels=batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an LSTM with ```pytorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file [LSTM.py](../src/LSTM.py), I've aready created an LSTM for you using ```pytorch```. Take some time to read through the code and make sure you understand how it's built up.\n",
    "\n",
    "Some questions for you to discuss in groups:\n",
    "\n",
    "- How is an LSTM layer created using ```pytorch```? How does the code compare to the classifier code we used last week?\n",
    "- What's going on with that weird bit that says ```@staticmethod```?\n",
    "  - [This might help](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "- On the forward pass, we use ```log_softmax()``` to make output predictions. What is this, and how does it relate to the output from the sigmoid function that we used for classification?\n",
    "- How would we make this LSTM model *bidirectional* - i.e. make it a Bi-LSTM? \n",
    "  - Hint: Check the documentation for the LSTM layer on the ```pytorch``` website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the LSTM for named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last part of the notebook, we are going to use bits of code we have seen today (related to batching) and code from last week to set up training and evaluation for an LSTM doing named entity recognition. There are a few parts of the code you have to fill: work in groups and note that there is nothing new you need to do! All you are required to do has been done earlier in this notebook, or last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Package all the code from previous steps into a single function, which prepares a batch of data for the model (we will apply this to all batches in the following chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idx(tokens, vocab=model.key_to_index):\n",
    "    \"\"\"\n",
    "    The function takes the tokens in our dataset and replaces it with the corresponding index from the GloVe vocabulary (vocab).\n",
    "    The function converts the tokens to lowercase to match the form of the vocabulary\n",
    "    If the word/token is not in the vocabulary, it yield the index matching \"UNK\" (unknown) which in this case is 40000\n",
    "    - What does the .get method do?\n",
    "    The get method tries to retrive the index for the token in lowercase\n",
    "    - Why lowercase?\n",
    "    We want the index to be the same for the same word regardless of whether it is in upper or lowercase\n",
    "    e.g. because it is in the beginning of a sentence\n",
    "    \"\"\"\n",
    "    return [vocab.get(t.lower(), vocab[\"UNK\"]) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(tokens, labels, vocab) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Prepare a batch of data for training.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[List[str]]): A list of lists of tokens.\n",
    "        labels (List[List[int]]): A list of lists of labels.\n",
    "        vocab (dict): A dictionary defining the model's vocabulary\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: A tuple of tensors containing the tokens and labels.\n",
    "    \"\"\"\n",
    "    batch_size = len(tokens)\n",
    "\n",
    "    # TODO: convert tokens to vocabulary items using the tokens_to_idx function\n",
    "    batch_tok_idx = [tokens_to_idx(sent) for sent in tokens]\n",
    "    \n",
    "    # TODO: compute length of longest sentence in batch.\n",
    "    batch_max_len = max([len(s) for s in batch_tok_idx])\n",
    "\n",
    "    # TODO: Pad the data and flag padded vs non-padded with 9\n",
    "    batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len))\n",
    "    batch_labels = 9 * np.ones((batch_size, batch_max_len))\n",
    "\n",
    "    # TODO: copy the data to numpy array\n",
    "    for i in range(batch_size):\n",
    "        tok_idx = batch_tok_idx[i] # Get the array i in the array of token indices\n",
    "        tags = labels[i] # Get tags for array i\n",
    "        size = len(tok_idx) # Get the length of the array \n",
    "\n",
    "        batch_input[i][:size] = tok_idx #replace part of the arrays of zeros with the right token indices -> This way, all sequences have the same size, with the surplus elements being zeros\n",
    "        batch_labels[i][:size] = tags\n",
    "    # TODO: convert data to tensors\n",
    "\n",
    "    # TODO: return two outputs: batch_input, batch_labels, tensors containing respectively tokens and true labels\n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that trains the model. \n",
    "\n",
    "**Questions**:\n",
    "What is the last part of the function doing?\n",
    "Think back of our first neural networks class: which problem is this trying to prevent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs, training, validation, vocab, patience, batch_size):\n",
    "    \"\"\"\n",
    "    A function for training the model.\n",
    "    \"\"\"\n",
    "    best_val_loss = None\n",
    "\n",
    "    # TODO: apply the prepare_batch function to the validation set, to get inputs and labels\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batches_processed = 0\n",
    "        print(f'*** Epoch {epoch+1} ***')\n",
    "        \n",
    "        # TODO: pass training['tokens'] and training['ner_tags'] them through the batch() function: call the outputs b_tokens and b_tags\n",
    "    \n",
    "        for tokens, tags in zip(b_tokens, b_tags):\n",
    "            batches_processed += 1\n",
    "            if batches_processed % 10 == 0:\n",
    "                print(f'Batch {batches_processed}')\n",
    "                \n",
    "            # prepare data\n",
    "            # TODO: use the prepare_batch function to create inputs and outputs\n",
    "\n",
    "            # train the model\n",
    "            #TODO: perform a forward pass\n",
    "            #TODO: compute the loss using the loss_fn method from our model (take a look at src/LSTM.py)\n",
    "            #TODO: compute the gradients\n",
    "            #TODO: update the weights (hint: look at notebook from last week!)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        #  periodically calculate loss on validation set\n",
    "        if epoch % 5 == 0:\n",
    "            #TODO: perform a forward pass on the validation set\n",
    "            #TODO: compute the loss on the validation set (call it \"loss\")\n",
    "\n",
    "            # QUESTION: what is this part of the code doing?\n",
    "            if best_val_loss is None or loss < best_val_loss:\n",
    "                best_val_loss = loss\n",
    "                torch.save(model, 'model.pt')\n",
    "                patience_ = patience\n",
    "            else:\n",
    "                patience_ -= 5\n",
    "                if patience_ <= 0:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that runs the whole thing (training and evaluation) end-to-end. Take some time to understand what this function does, and note down any questions you might have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(gensim_embedding: str, batch_size: int, epochs: int, learning_rate: float, patience: int = 10, optimizer=0):\n",
    "    \"\"\"\n",
    "    A function that does end-to-end data prepraration, training, and evaluation\n",
    "    \"\"\"\n",
    "    # set a seed to make the results reproducible\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    # use the function gensim_to_torch_embeddings to create embedding_layer and vocab\n",
    "    embeddings = api.load(gensim_embedding)\n",
    "    embedding_layer, vocab = gensim_to_torch_embedding(embeddings)\n",
    "\n",
    "    # Preparing data\n",
    "    # shuffle dataset\n",
    "    dataset = load_dataset(\"conllpp\")\n",
    "    train = dataset[\"train\"].shuffle(seed=1)\n",
    "    test = dataset[\"test\"]\n",
    "    validation = dataset[\"validation\"]\n",
    "\n",
    "    # Compute the number of classes for LSTM output (+1 for PAD)\n",
    "    num_classes = train.features[\"ner_tags\"].feature.num_classes\n",
    "\n",
    "    # Initialize the model\n",
    "    lstm = LSTMModel(num_classes + 1, embedding_layer, 20)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer == 0:\n",
    "        optimizer = torch.optim.AdamW(lstm.parameters(), lr=learning_rate)\n",
    "    elif optimizer == 1:\n",
    "        optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train model with given settings\n",
    "    train_model(lstm, optimizer, epochs, train, validation, vocab, patience, batch_size)\n",
    "\n",
    "    # Load the best model\n",
    "    best = torch.load('model.pt')\n",
    "\n",
    "    # test it on test set\n",
    "    X, y = prepare_batch(test[\"tokens\"], test[\"ner_tags\"], vocab)\n",
    "    y_hat = best.predict(X)\n",
    "\n",
    "    # reformat results by removing pad tokens and flattening\n",
    "    y_hat_depadded = []\n",
    "    pos = 0\n",
    "    for sen in test[\"ner_tags\"]:\n",
    "        for i in range(pos, pos + len(sen)):\n",
    "            y_hat_depadded.append(y_hat[i])\n",
    "        pos += y.shape[1]\n",
    "\n",
    "    # flatten the test sentences into a single list\n",
    "    flat_tags = [item for sublist in test[\"ner_tags\"] for item in sublist]\n",
    "    \n",
    "    # get actual label\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    ner_dict = {0:'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC', 9: 'NONE'}\n",
    "    # in theory we would want to exclude label 9 -- but let's keep it in for simplicity\n",
    "    actual.append([ner_dict.get(k, k) for k in flat_tags])\n",
    "    predicted.append([ner_dict.get(k, k) for k in y_hat_depadded])\n",
    "\n",
    "    # calculate f1 and acc (currently the same number?)\n",
    "    report = classification_report(actual, predicted)\n",
    "    print(report)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the run function to train and evaluate the model\n",
    "# hint: start with learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "- How do results change between bidirectional and unidirectional models?\n",
    "- How does the size of the LSTM affect performance?\n",
    "- Is the performance of the model balanced for all classes?\n",
    "\n",
    "### Bonus task:\n",
    "- If you want to evaluate performance as a function of parameters systematically, try implement all this through scripts, and log results as separate files outputs\n",
    "- A good way to monitor training is by using Weights&Biases. Check out their documentation and feel free to experiment: https://docs.wandb.ai/guides/integrations/pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-e23",
   "language": "python",
   "name": "nlp-e23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
